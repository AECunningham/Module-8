---
title: "Module 8 Project"
author: "Anne Cunningham"
date: "26/04/2015"
output: html_document
---
Human Activity Monitoring - predicting how well exercises are done
==================================================================

## Introduction
This project relates to an experiment in which six participants were asked to perform barbell lifts, firstly in the correct manner (coded as "A"), and then in each of four incorrect ways (coded "B" to "E"). Their movements were tracked by taking multiple sets of positional measurements at frequent intervals from accelerometers on their belt, arm, forearm and the dumbbell. 

The challenge consists of trying to predict the type of barbell lift (i.e. A, B, C, D or E) which the subject was performing, given just one record (i.e. a snapshot of their readings at a single point in time).

## Exploring the data
The data for this exercise comes in the form of a 'training' dataset of 19622 records, and a 'testing' dataset of just 20 records. 
```{r}
library(RCurl)
myCsv <- getURL("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")
projtrain <- read.csv(textConnection(myCsv),na.strings=c("#DIV/0!","NA"))
myCsv <- getURL("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv")
projtest <- read.csv(textConnection(myCsv),na.strings=c("#DIV/0!","NA"))
```
The training dataset consists of 160 variables (i.e. columns). As is common practice in such experiments, successive records are grouped into 'sliding windows', each lasting a set number of seconds. 

Upon inspecting the training data in Excel, it became apparent that there were many derived columns, containing summary quantities such as the average, skewness or kurtosis of one of the actual measurements. These derived columns were only filled in once per 'window', so they were empty or missing for the vast majority of records (and even then they did not always seem to be correctly calculated). 

The derived fields were also absent from all the records in the 'testing' dataset, so it was decided to eliminate them from the modelling process. This reduced the total number of variables to 60. It was also decided to omit the first seven variables, which consisted of identifiers for the record, subject, time and window. This left 52 predictor variables, plus the 'classe' variable which we were trying to predict.

```{r}
notwanted <- "^kurtosis|^skewness|^max|^min|^amplitude|^var|^avg|^stddev"
projslim <- subset(projtrain,select = 
        names(projtrain)[-grep(notwanted,names(projtrain))])[c(-1,-2,-3,-4,-5,-6,-7)]
```
## Developing the prediction model
For the purposes of developing the model, it was decided to further split the large 'training' set into training and testing partitions, of 70% and 30% respectively:
```{r}
library(caret)
set.seed(3456)
inTrain <- createDataPartition(projslim$classe,p=0.7,list=FALSE)
training <- projslim[inTrain,]
testing <- projslim[-inTrain,]
```
The lecture notes, the discussion boards and the paper accompanying the data all pointed to **random forest** modelling as a suitable prediction tool for this data.

Initial attempts were made to run a random forest model on the training data using the caret package, but it soon became clear that this would take far too long on a rather modest computer. Instead, the randomForest function was used, and this produced an answer in fifteen minutes:
```{r trainmodel,cache=TRUE}
library(randomForest)
set.seed(3456)
rf.modFit <- randomForest(classe ~ .,data=training,importance=TRUE)
rf.modFit
```
The model had an 'Out-of-Bag' error rate of less than half a percent, which was a highly encouraging result. The OOB error rate is arrived at by testing each tree on the training observations which were *not* used to create it, so it gives us a good idea of what the test error will be.

It is also now possible to examine which variables are the most important predictors, by drawing a graph:
```{r varimpplot,fig.height=4}
 varImpPlot(rf.modFit,cex=0.6)
```

The left- and right-hand charts give slightly different results, but they are both agreed on the three most important variables.

It is also possible to see how the accuracy of prediction varies according to the number of trees generated by the model. We left the number of trees at its default value of 500, but it can be seen that anything above about 350 would have generated equally good results:

```{r modelfitplot,fig.height=4}
plot(rf.modFit,log="y")
legend("topright",legend=unique(training$classe),
       col=unique(as.numeric(training$classe)),pch=19)
```

The next step was to run the model on the held-out part of the main dataset, to see how it performed on data that had not been used to train it. The results were equally impressive:
```{r}
yhat.rf =  predict(rf.modFit,newdata=testing)
confusionMatrix(yhat.rf,testing$classe)
```
Finally, the model was run on the 20 records from the separate testing dataset. The results (not shown here) were submitted via the Coursera interface, and all turned out to predict the correct class (A, B, C, D or E).
```{r finaltest,results="hide"}
predict20 <- predict(rf.modFit,newdata=projtest)
predict20
```
## Too Good To Be True?
Of course it is highly satisfying to be able to predict 20 out of 20 results correctly - but is it a bit too good to be true? It is notable that all the 20 records in the testing dataset related to the same six subjects as before. It is not at all clear how successful we would be in predicting the results for anybody else - i.e. a subject upon whom the prediction algorithm had not been trained.

The identity of the subject was *not* used as a parameter when fitting the model. However, some of the variables seem to behave very differently depending on who the subject is. The 'roll_belt' parameter, for example, is close to zero for some people, whichever activity they may be doing (i.e. A, B, C, D or E), but well over 100 for other people. This is graphically illustrated in the chart below:
```{r discrepancies,fig.height=6}
library(ggplot2)
ggplot(projtrain,aes(X,roll_belt,colour=factor(classe)))+geom_point() +
facet_wrap(~user_name,nrow=1) +
theme(axis.ticks.x = element_blank(),axis.text.x = element_blank(),strip.text.x = element_text(size=12)) +
labs(x="") + scale_color_discrete(name="classe")
```

Without being an expert on the type of data being collected, it is hard to know why this is happening. Perhaps the roll_belt variable is an angle, and half the people were facing one way when they did the exercises, and the other half were facing in a different direction! The model has coped very well with this dichotomy, despite not knowing who is who. However, suppose a new, seventh person came along, whose readings on this variable were completely different - say around 80. The model would probably fail spectacularly if asked to make a prediction for him. In that sense, it has been over-fitted to the peculiarities of the six people who were represented in the training dataset.